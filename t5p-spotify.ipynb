{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de filas: 100144\n",
      "cantidad de columnas: 13\n",
      "Total de valores faltantes en el dataset: 1.56 %\n",
      "Porcentje de falsos: 47.32385365074293 %\n",
      "Cantidad de valores faltanes por columna: Unnamed: 0                               0\n",
      "ts                                       0\n",
      "username                                 0\n",
      "platform                                 0\n",
      "conn_country                             0\n",
      "user_agent_decrypted                 16397\n",
      "master_metadata_track_name             973\n",
      "master_metadata_album_artist_name      973\n",
      "master_metadata_album_album_name       973\n",
      "spotify_track_uri                      973\n",
      "reason_start                             4\n",
      "shuffle                                  0\n",
      "TARGET                                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "canciones = pd.read_csv(\"competition_data.csv\")\n",
    "print(f'cantidad de filas: {canciones.shape[0]}')\n",
    "print(f'cantidad de columnas: {canciones.shape[1]}')\n",
    "cant_nan = (canciones.isnull().sum().sum()/(canciones.shape[0]*canciones.shape[1]))*100\n",
    "print(f\"Total de valores faltantes en el dataset: {round(cant_nan,2)} %\")\n",
    "#balanceados los datos?\n",
    "\n",
    "porcentaje_false = ((canciones[canciones[\"TARGET\"]==0]).shape[0]/canciones.shape[0])*100\n",
    "print(f\"Porcentje de falsos: {porcentaje_false} %\")\n",
    "print(f\"Cantidad de valores faltanes por columna: {canciones.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir las columnas a verificar\n",
    "columnas_objetivo = [\n",
    "    \"master_metadata_track_name\",\n",
    "    \"master_metadata_album_artist_name\",\n",
    "    \"master_metadata_album_album_name\"\n",
    "]\n",
    "\n",
    "# Filtrar filas donde las tres columnas sean NaN\n",
    "filas_faltantes = canciones[canciones[columnas_objetivo].isnull().all(axis=1)]\n",
    "\n",
    "# Seleccionar solo las columnas deseadas (las tres columnas + user_id)\n",
    "columnas_a_mostrar = columnas_objetivo + [\"username\"]\n",
    "resultado = filas_faltantes[columnas_a_mostrar]\n",
    "\n",
    "# Mostrar resultados\n",
    "#print(resultado)\n",
    "#print(f\"Cantidad de filas donde faltan los tres atributos: {resultado.shape[0]}\")\n",
    "#print(canciones.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.20.3)\n",
      "Requirement already satisfied: scipy in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.7.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 36926, number of negative: 33174\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6438\n",
      "[LightGBM] [Info] Number of data points in the train set: 70100, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.526762 -> initscore=0.107149\n",
      "[LightGBM] [Info] Start training from score 0.107149\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9054\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "########PREPARACION DE DATOS\n",
    "\n",
    "# Convertir columnas categóricas\n",
    "categorical_cols = [\n",
    "    \"platform\", \"conn_country\", \"user_agent_decrypted\",\n",
    "    \"master_metadata_track_name\", \"master_metadata_album_artist_name\",\n",
    "    \"master_metadata_album_album_name\", \"reason_start\"\n",
    "]\n",
    "\n",
    "\n",
    "for col in categorical_cols:\n",
    "    canciones[col] = canciones[col].astype(\"category\")\n",
    "\n",
    "# Convertir fechas\n",
    "canciones[\"ts\"] = pd.to_datetime(canciones[\"ts\"], utc=True).view(\"int64\") / 10**9\n",
    "\n",
    "\n",
    "# Eliminar \"spotify_track_uri\" si no es relevante (parece un identificador único)\n",
    "if \"spotify_track_uri\" in canciones.columns:\n",
    "    canciones = canciones.drop(columns=[\"spotify_track_uri\"])\n",
    "\n",
    "canciones.columns = (\n",
    "    canciones.columns\n",
    "    .str.replace(r\"[^\\w]\", \"_\", regex=True)  # Reemplaza cualquier carácter no alfanumérico por \"_\"\n",
    "    .str.lower()  # Convierte los nombres a minúsculas\n",
    ")\n",
    "\n",
    "########MODELO\n",
    "# Separar variables predictoras (X) y target (y)\n",
    "y = canciones[\"target\"]\n",
    "X = canciones.drop(columns=[\"target\"])  \n",
    "\n",
    "\n",
    "# Primera división: 70% Train, 30% (que luego se dividirá en valid y test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Segunda división: dividir el 30% restante en 15% validación y 15% test\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp) #cheqiuear qwue es stratify (NO SE SABE SI ESTA BIENE ESTO!!!!!)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "\n",
    "params_lgb = {\n",
    "    'num_leaves': 2**10,  # Relacionado con la profundidad, ajusta si sigue sobreajustando\n",
    "    'learning_rate': 0.1, \n",
    "    'max_depth': 10,  # Equivalente a 'depth' en CatBoost\n",
    "    'objective': 'binary', \n",
    "    'metric': 'auc',\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "num_round = 10\n",
    "modelo = lgb.train(param, train_data, num_round, valid_sets = [val_data])\n",
    "\n",
    "# Predecir probabilidades en el conjunto de prueba\n",
    "y_pred_proba = modelo.predict(X_test)\n",
    "\n",
    "# Calcular la Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar la Curva ROC\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Línea diagonal\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC - LightGBM (Default)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (3.4.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.24 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (1.3.4)\n",
      "Requirement already satisfied: scipy in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (1.7.3)\n",
      "Requirement already satisfied: plotly in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: six in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->catboost) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from plotly->catboost) (1.33.0)\n",
      "Requirement already satisfied: packaging in /Users/catalinachablopez/opt/anaconda3/lib/python3.9/site-packages (from plotly->catboost) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.8878059205\n",
      "bestIteration = 97\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "0:\tloss: 0.8878059\tbest: 0.8878059 (0)\ttotal: 3.08s\tremaining: 5m 29s\n",
      "\n",
      "bestTest = 0.9003899372\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "1:\tloss: 0.9003899\tbest: 0.9003899 (1)\ttotal: 6.39s\tremaining: 5m 38s\n",
      "\n",
      "bestTest = 0.9032882384\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "2:\tloss: 0.9032882\tbest: 0.9032882 (2)\ttotal: 9.54s\tremaining: 5m 33s\n",
      "\n",
      "bestTest = 0.8881086583\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "3:\tloss: 0.8881087\tbest: 0.9032882 (2)\ttotal: 12.7s\tremaining: 5m 30s\n",
      "\n",
      "bestTest = 0.9005073796\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "4:\tloss: 0.9005074\tbest: 0.9032882 (2)\ttotal: 16s\tremaining: 5m 29s\n",
      "\n",
      "bestTest = 0.9036778982\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "5:\tloss: 0.9036779\tbest: 0.9036779 (5)\ttotal: 19.2s\tremaining: 5m 26s\n",
      "\n",
      "bestTest = 0.8877019996\n",
      "bestIteration = 98\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "6:\tloss: 0.8877020\tbest: 0.9036779 (5)\ttotal: 22.4s\tremaining: 5m 22s\n",
      "\n",
      "bestTest = 0.9006561279\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "7:\tloss: 0.9006561\tbest: 0.9036779 (5)\ttotal: 25.5s\tremaining: 5m 18s\n",
      "\n",
      "bestTest = 0.9033430795\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "8:\tloss: 0.9033431\tbest: 0.9036779 (5)\ttotal: 28.7s\tremaining: 5m 15s\n",
      "\n",
      "bestTest = 0.8878537255\n",
      "bestIteration = 98\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "9:\tloss: 0.8878537\tbest: 0.9036779 (5)\ttotal: 32.5s\tremaining: 5m 18s\n",
      "\n",
      "bestTest = 0.9008338263\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "10:\tloss: 0.9008338\tbest: 0.9036779 (5)\ttotal: 35.9s\tremaining: 5m 16s\n",
      "\n",
      "bestTest = 0.9034704542\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "11:\tloss: 0.9034705\tbest: 0.9036779 (5)\ttotal: 39.4s\tremaining: 5m 15s\n",
      "\n",
      "bestTest = 0.903334269\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "12:\tloss: 0.9033343\tbest: 0.9036779 (5)\ttotal: 46.9s\tremaining: 5m 42s\n",
      "\n",
      "bestTest = 0.909607188\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "13:\tloss: 0.9096072\tbest: 0.9096072 (13)\ttotal: 55.5s\tremaining: 6m 12s\n",
      "\n",
      "bestTest = 0.9115919628\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "14:\tloss: 0.9115920\tbest: 0.9115920 (14)\ttotal: 1m 4s\tremaining: 6m 38s\n",
      "\n",
      "bestTest = 0.9033232253\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "15:\tloss: 0.9033232\tbest: 0.9115920 (14)\ttotal: 1m 11s\tremaining: 6m 51s\n",
      "\n",
      "bestTest = 0.9099475744\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "16:\tloss: 0.9099476\tbest: 0.9115920 (14)\ttotal: 1m 20s\tremaining: 7m 10s\n",
      "\n",
      "bestTest = 0.9114449787\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "17:\tloss: 0.9114450\tbest: 0.9115920 (14)\ttotal: 1m 29s\tremaining: 7m 28s\n",
      "\n",
      "bestTest = 0.9035002201\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "18:\tloss: 0.9035002\tbest: 0.9115920 (14)\ttotal: 1m 37s\tremaining: 7m 35s\n",
      "\n",
      "bestTest = 0.9096146015\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "19:\tloss: 0.9096146\tbest: 0.9115920 (14)\ttotal: 1m 46s\tremaining: 7m 48s\n",
      "\n",
      "bestTest = 0.9116026088\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "20:\tloss: 0.9116026\tbest: 0.9116026 (20)\ttotal: 1m 55s\tremaining: 7m 58s\n",
      "\n",
      "bestTest = 0.9035206556\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "21:\tloss: 0.9035207\tbest: 0.9116026 (20)\ttotal: 2m 3s\tremaining: 8m 3s\n",
      "\n",
      "bestTest = 0.9097607392\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "22:\tloss: 0.9097607\tbest: 0.9116026 (20)\ttotal: 2m 12s\tremaining: 8m 11s\n",
      "\n",
      "bestTest = 0.9116090841\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "23:\tloss: 0.9116091\tbest: 0.9116091 (23)\ttotal: 2m 22s\tremaining: 8m 19s\n",
      "\n",
      "bestTest = 0.9057036763\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "24:\tloss: 0.9057037\tbest: 0.9116091 (23)\ttotal: 2m 34s\tremaining: 8m 32s\n",
      "\n",
      "bestTest = 0.9107480432\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "25:\tloss: 0.9107480\tbest: 0.9116091 (23)\ttotal: 2m 49s\tremaining: 8m 55s\n",
      "\n",
      "bestTest = 0.9123392118\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "26:\tloss: 0.9123392\tbest: 0.9123392 (26)\ttotal: 3m 4s\tremaining: 9m 14s\n",
      "\n",
      "bestTest = 0.9056570542\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "27:\tloss: 0.9056571\tbest: 0.9123392 (26)\ttotal: 3m 17s\tremaining: 9m 23s\n",
      "\n",
      "bestTest = 0.9112413893\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "28:\tloss: 0.9112414\tbest: 0.9123392 (26)\ttotal: 3m 33s\tremaining: 9m 41s\n",
      "\n",
      "bestTest = 0.9122992995\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "29:\tloss: 0.9122993\tbest: 0.9123392 (26)\ttotal: 3m 48s\tremaining: 9m 53s\n",
      "\n",
      "bestTest = 0.9056324379\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "30:\tloss: 0.9056324\tbest: 0.9123392 (26)\ttotal: 3m 59s\tremaining: 9m 55s\n",
      "\n",
      "bestTest = 0.9108297951\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "31:\tloss: 0.9108298\tbest: 0.9123392 (26)\ttotal: 4m 12s\tremaining: 10m\n",
      "\n",
      "bestTest = 0.9126209126\n",
      "bestIteration = 297\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "32:\tloss: 0.9126209\tbest: 0.9126209 (32)\ttotal: 4m 26s\tremaining: 10m 6s\n",
      "\n",
      "bestTest = 0.9058801918\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "33:\tloss: 0.9058802\tbest: 0.9126209 (32)\ttotal: 4m 39s\tremaining: 10m 7s\n",
      "\n",
      "bestTest = 0.9108971585\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "34:\tloss: 0.9108972\tbest: 0.9126209 (32)\ttotal: 4m 54s\tremaining: 10m 14s\n",
      "\n",
      "bestTest = 0.9124243492\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "35:\tloss: 0.9124243\tbest: 0.9126209 (32)\ttotal: 5m 10s\tremaining: 10m 20s\n",
      "\n",
      "bestTest = 0.8921450717\n",
      "bestIteration = 88\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "36:\tloss: 0.8921451\tbest: 0.9126209 (32)\ttotal: 5m 14s\tremaining: 10m 3s\n",
      "\n",
      "bestTest = 0.9036574729\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "37:\tloss: 0.9036575\tbest: 0.9126209 (32)\ttotal: 5m 19s\tremaining: 9m 47s\n",
      "\n",
      "bestTest = 0.9062776832\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "38:\tloss: 0.9062777\tbest: 0.9126209 (32)\ttotal: 5m 23s\tremaining: 9m 32s\n",
      "\n",
      "bestTest = 0.8924210926\n",
      "bestIteration = 98\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "39:\tloss: 0.8924211\tbest: 0.9126209 (32)\ttotal: 5m 27s\tremaining: 9m 17s\n",
      "\n",
      "bestTest = 0.9035040645\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "40:\tloss: 0.9035041\tbest: 0.9126209 (32)\ttotal: 5m 32s\tremaining: 9m 2s\n",
      "\n",
      "bestTest = 0.9059709683\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "41:\tloss: 0.9059710\tbest: 0.9126209 (32)\ttotal: 5m 36s\tremaining: 8m 49s\n",
      "\n",
      "bestTest = 0.892513623\n",
      "bestIteration = 97\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "42:\tloss: 0.8925136\tbest: 0.9126209 (32)\ttotal: 5m 40s\tremaining: 8m 35s\n",
      "\n",
      "bestTest = 0.9036257491\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "43:\tloss: 0.9036257\tbest: 0.9126209 (32)\ttotal: 5m 44s\tremaining: 8m 21s\n",
      "\n",
      "bestTest = 0.9063076429\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "44:\tloss: 0.9063076\tbest: 0.9126209 (32)\ttotal: 5m 48s\tremaining: 8m 8s\n",
      "\n",
      "bestTest = 0.8923153872\n",
      "bestIteration = 98\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "45:\tloss: 0.8923154\tbest: 0.9126209 (32)\ttotal: 5m 52s\tremaining: 7m 55s\n",
      "\n",
      "bestTest = 0.9037586711\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "46:\tloss: 0.9037587\tbest: 0.9126209 (32)\ttotal: 5m 56s\tremaining: 7m 43s\n",
      "\n",
      "bestTest = 0.9061546016\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "47:\tloss: 0.9061546\tbest: 0.9126209 (32)\ttotal: 6m 1s\tremaining: 7m 31s\n",
      "\n",
      "bestTest = 0.9053360835\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "48:\tloss: 0.9053361\tbest: 0.9126209 (32)\ttotal: 6m 11s\tremaining: 7m 27s\n",
      "\n",
      "bestTest = 0.9111421184\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "49:\tloss: 0.9111421\tbest: 0.9126209 (32)\ttotal: 6m 23s\tremaining: 7m 25s\n",
      "\n",
      "bestTest = 0.9130821576\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "50:\tloss: 0.9130822\tbest: 0.9130822 (50)\ttotal: 6m 36s\tremaining: 7m 23s\n",
      "\n",
      "bestTest = 0.9060765717\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "51:\tloss: 0.9060766\tbest: 0.9130822 (50)\ttotal: 6m 46s\tremaining: 7m 17s\n",
      "\n",
      "bestTest = 0.9113484917\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "52:\tloss: 0.9113485\tbest: 0.9130822 (50)\ttotal: 6m 58s\tremaining: 7m 14s\n",
      "\n",
      "bestTest = 0.9130786599\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "53:\tloss: 0.9130787\tbest: 0.9130822 (50)\ttotal: 7m 10s\tremaining: 7m 10s\n",
      "\n",
      "bestTest = 0.9058228931\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "54:\tloss: 0.9058229\tbest: 0.9130822 (50)\ttotal: 7m 20s\tremaining: 7m 4s\n",
      "\n",
      "bestTest = 0.9114116946\n",
      "bestIteration = 196\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "55:\tloss: 0.9114117\tbest: 0.9130822 (50)\ttotal: 7m 32s\tremaining: 7m\n",
      "\n",
      "bestTest = 0.9129619924\n",
      "bestIteration = 194\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "56:\tloss: 0.9129620\tbest: 0.9130822 (50)\ttotal: 7m 45s\tremaining: 6m 56s\n",
      "\n",
      "bestTest = 0.9055926276\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "57:\tloss: 0.9055926\tbest: 0.9130822 (50)\ttotal: 7m 54s\tremaining: 6m 49s\n",
      "\n",
      "bestTest = 0.9111897807\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "58:\tloss: 0.9111898\tbest: 0.9130822 (50)\ttotal: 8m 6s\tremaining: 6m 44s\n",
      "\n",
      "bestTest = 0.9130888776\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "59:\tloss: 0.9130889\tbest: 0.9130889 (59)\ttotal: 8m 19s\tremaining: 6m 39s\n",
      "\n",
      "bestTest = 0.9076678933\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "60:\tloss: 0.9076679\tbest: 0.9130889 (59)\ttotal: 8m 36s\tremaining: 6m 37s\n",
      "\n",
      "bestTest = 0.9123513262\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "61:\tloss: 0.9123513\tbest: 0.9130889 (59)\ttotal: 8m 54s\tremaining: 6m 36s\n",
      "\n",
      "bestTest = 0.9140458446\n",
      "bestIteration = 298\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "62:\tloss: 0.9140458\tbest: 0.9140458 (62)\ttotal: 9m 14s\tremaining: 6m 36s\n",
      "\n",
      "bestTest = 0.9081856925\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "63:\tloss: 0.9081857\tbest: 0.9140458 (62)\ttotal: 9m 30s\tremaining: 6m 32s\n",
      "\n",
      "bestTest = 0.9125094254\n",
      "bestIteration = 298\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "64:\tloss: 0.9125094\tbest: 0.9140458 (62)\ttotal: 9m 49s\tremaining: 6m 29s\n",
      "\n",
      "bestTest = 0.9140635267\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "65:\tloss: 0.9140635\tbest: 0.9140635 (65)\ttotal: 10m 8s\tremaining: 6m 27s\n",
      "\n",
      "bestTest = 0.9080178855\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "66:\tloss: 0.9080179\tbest: 0.9140635 (65)\ttotal: 10m 23s\tremaining: 6m 21s\n",
      "\n",
      "bestTest = 0.9125692938\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "67:\tloss: 0.9125693\tbest: 0.9140635 (65)\ttotal: 10m 41s\tremaining: 6m 17s\n",
      "\n",
      "bestTest = 0.913982193\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "68:\tloss: 0.9139822\tbest: 0.9140635 (65)\ttotal: 11m\tremaining: 6m 13s\n",
      "\n",
      "bestTest = 0.907779666\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "69:\tloss: 0.9077797\tbest: 0.9140635 (65)\ttotal: 11m 16s\tremaining: 6m 7s\n",
      "\n",
      "bestTest = 0.912146146\n",
      "bestIteration = 298\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "70:\tloss: 0.9121461\tbest: 0.9140635 (65)\ttotal: 11m 34s\tremaining: 6m 1s\n",
      "\n",
      "bestTest = 0.9139126065\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "71:\tloss: 0.9139126\tbest: 0.9140635 (65)\ttotal: 11m 52s\tremaining: 5m 56s\n",
      "\n",
      "bestTest = 0.8938974496\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "72:\tloss: 0.8938974\tbest: 0.9140635 (65)\ttotal: 11m 57s\tremaining: 5m 43s\n",
      "\n",
      "bestTest = 0.9058015807\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "73:\tloss: 0.9058016\tbest: 0.9140635 (65)\ttotal: 12m 1s\tremaining: 5m 31s\n",
      "\n",
      "bestTest = 0.9083242332\n",
      "bestIteration = 98\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "74:\tloss: 0.9083242\tbest: 0.9140635 (65)\ttotal: 12m 6s\tremaining: 5m 19s\n",
      "\n",
      "bestTest = 0.8940310854\n",
      "bestIteration = 96\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "75:\tloss: 0.8940311\tbest: 0.9140635 (65)\ttotal: 12m 11s\tremaining: 5m 7s\n",
      "\n",
      "bestTest = 0.9054386583\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "76:\tloss: 0.9054387\tbest: 0.9140635 (65)\ttotal: 12m 15s\tremaining: 4m 56s\n",
      "\n",
      "bestTest = 0.9075615863\n",
      "bestIteration = 97\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "77:\tloss: 0.9075616\tbest: 0.9140635 (65)\ttotal: 12m 20s\tremaining: 4m 44s\n",
      "\n",
      "bestTest = 0.8939484668\n",
      "bestIteration = 97\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "78:\tloss: 0.8939485\tbest: 0.9140635 (65)\ttotal: 12m 25s\tremaining: 4m 33s\n",
      "\n",
      "bestTest = 0.9057713559\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "79:\tloss: 0.9057714\tbest: 0.9140635 (65)\ttotal: 12m 29s\tremaining: 4m 22s\n",
      "\n",
      "bestTest = 0.9077550905\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "80:\tloss: 0.9077551\tbest: 0.9140635 (65)\ttotal: 12m 34s\tremaining: 4m 11s\n",
      "\n",
      "bestTest = 0.8941887869\n",
      "bestIteration = 98\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "81:\tloss: 0.8941888\tbest: 0.9140635 (65)\ttotal: 12m 38s\tremaining: 4m\n",
      "\n",
      "bestTest = 0.9058066284\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "82:\tloss: 0.9058066\tbest: 0.9140635 (65)\ttotal: 12m 42s\tremaining: 3m 49s\n",
      "\n",
      "bestTest = 0.9079402534\n",
      "bestIteration = 99\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "83:\tloss: 0.9079403\tbest: 0.9140635 (65)\ttotal: 12m 47s\tremaining: 3m 39s\n",
      "\n",
      "bestTest = 0.906188783\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "84:\tloss: 0.9061888\tbest: 0.9140635 (65)\ttotal: 13m\tremaining: 3m 31s\n",
      "\n",
      "bestTest = 0.9125836822\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "85:\tloss: 0.9125837\tbest: 0.9140635 (65)\ttotal: 13m 16s\tremaining: 3m 23s\n",
      "\n",
      "bestTest = 0.9132068299\n",
      "bestIteration = 188\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "86:\tloss: 0.9132068\tbest: 0.9140635 (65)\ttotal: 13m 32s\tremaining: 3m 16s\n",
      "\n",
      "bestTest = 0.9066619996\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "87:\tloss: 0.9066620\tbest: 0.9140635 (65)\ttotal: 13m 44s\tremaining: 3m 7s\n",
      "\n",
      "bestTest = 0.9126584999\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "88:\tloss: 0.9126585\tbest: 0.9140635 (65)\ttotal: 14m\tremaining: 2m 59s\n",
      "\n",
      "bestTest = 0.9140286825\n",
      "bestIteration = 190\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "89:\tloss: 0.9140287\tbest: 0.9140635 (65)\ttotal: 14m 19s\tremaining: 2m 51s\n",
      "\n",
      "bestTest = 0.9068440726\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "90:\tloss: 0.9068441\tbest: 0.9140635 (65)\ttotal: 14m 32s\tremaining: 2m 43s\n",
      "\n",
      "bestTest = 0.9125677642\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "91:\tloss: 0.9125678\tbest: 0.9140635 (65)\ttotal: 14m 47s\tremaining: 2m 34s\n",
      "\n",
      "bestTest = 0.913628856\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "92:\tloss: 0.9136289\tbest: 0.9140635 (65)\ttotal: 15m 5s\tremaining: 2m 25s\n",
      "\n",
      "bestTest = 0.9068376789\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "93:\tloss: 0.9068377\tbest: 0.9140635 (65)\ttotal: 15m 18s\tremaining: 2m 16s\n",
      "\n",
      "bestTest = 0.9126038423\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "94:\tloss: 0.9126038\tbest: 0.9140635 (65)\ttotal: 15m 33s\tremaining: 2m 7s\n",
      "\n",
      "bestTest = 0.9135320427\n",
      "bestIteration = 193\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "95:\tloss: 0.9135320\tbest: 0.9140635 (65)\ttotal: 15m 49s\tremaining: 1m 58s\n",
      "\n",
      "bestTest = 0.9087559774\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "96:\tloss: 0.9087560\tbest: 0.9140635 (65)\ttotal: 16m 9s\tremaining: 1m 49s\n",
      "\n",
      "bestTest = 0.9138062995\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "97:\tloss: 0.9138063\tbest: 0.9140635 (65)\ttotal: 16m 32s\tremaining: 1m 41s\n",
      "\n",
      "bestTest = 0.9139884337\n",
      "bestIteration = 285\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "98:\tloss: 0.9139884\tbest: 0.9140635 (65)\ttotal: 16m 56s\tremaining: 1m 32s\n",
      "\n",
      "bestTest = 0.9091989487\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "99:\tloss: 0.9091989\tbest: 0.9140635 (65)\ttotal: 17m 16s\tremaining: 1m 22s\n",
      "\n",
      "bestTest = 0.9135727708\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "100:\tloss: 0.9135728\tbest: 0.9140635 (65)\ttotal: 17m 40s\tremaining: 1m 13s\n",
      "\n",
      "bestTest = 0.9148365852\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "101:\tloss: 0.9148366\tbest: 0.9148366 (101)\ttotal: 18m 4s\tremaining: 1m 3s\n",
      "\n",
      "bestTest = 0.9089618815\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "102:\tloss: 0.9089619\tbest: 0.9148366 (101)\ttotal: 18m 25s\tremaining: 53.7s\n",
      "\n",
      "bestTest = 0.913675315\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "103:\tloss: 0.9136753\tbest: 0.9148366 (101)\ttotal: 18m 54s\tremaining: 43.6s\n",
      "\n",
      "bestTest = 0.9145181638\n",
      "bestIteration = 294\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "104:\tloss: 0.9145182\tbest: 0.9148366 (101)\ttotal: 19m 21s\tremaining: 33.2s\n",
      "\n",
      "bestTest = 0.9091136482\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "105:\tloss: 0.9091136\tbest: 0.9148366 (101)\ttotal: 19m 45s\tremaining: 22.4s\n",
      "\n",
      "bestTest = 0.9136969945\n",
      "bestIteration = 299\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "106:\tloss: 0.9136970\tbest: 0.9148366 (101)\ttotal: 20m 10s\tremaining: 11.3s\n",
      "\n",
      "bestTest = 0.9138780274\n",
      "bestIteration = 296\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "107:\tloss: 0.9138780\tbest: 0.9148366 (101)\ttotal: 20m 36s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.9119540392\n",
      "bestIteration = 296\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.9142741201\n",
      "bestIteration = 292\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.915114142\n",
      "bestIteration = 233\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 61\u001b[0m\n\u001b[1;32m     55\u001b[0m best_params \u001b[38;5;241m=\u001b[39m modelo\u001b[38;5;241m.\u001b[39mgrid_search(grid_params, X\u001b[38;5;241m=\u001b[39mX_train, y\u001b[38;5;241m=\u001b[39my_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, partition_random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m modelofinal \u001b[38;5;241m=\u001b[39m \u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCatBoostClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Entrenar con todo el dataset de entrenamiento\u001b[39;00m\n\u001b[1;32m     64\u001b[0m modelofinal\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m(X_valid, y_valid), early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'params'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######## PREPARACIÓN DE DATOS ########\n",
    "\n",
    "# Convertir columnas categóricas\n",
    "categorical_cols = [\n",
    "    \"platform\", \"conn_country\", \"user_agent_decrypted\",\n",
    "    \"master_metadata_track_name\", \"master_metadata_album_artist_name\",\n",
    "    \"master_metadata_album_album_name\", \"reason_start\"\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    canciones[col] = canciones[col].astype(str)\n",
    "\n",
    "# Convertir fechas\n",
    "canciones[\"ts\"] = pd.to_datetime(canciones[\"ts\"], utc=True).view(\"int64\") / 10**9\n",
    "\n",
    "\n",
    "# Eliminar \"spotify_track_uri\" si no es relevante\n",
    "if \"spotify_track_uri\" in canciones.columns:\n",
    "    canciones = canciones.drop(columns=[\"spotify_track_uri\"])\n",
    "\n",
    "# Normalizar nombres de columnas\n",
    "canciones.columns = (\n",
    "    canciones.columns\n",
    "    .str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "######## MODELO ########\n",
    "# Separar variables predictoras (X) y target (y)\n",
    "y = canciones[\"target\"]\n",
    "X = canciones.drop(columns=[\"target\"])\n",
    "\n",
    "# Dividir en conjunto de entrenamiento, validación y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Configurar el dataset para CatBoost\n",
    "train_data = cb.Pool(X_train, label=y_train, cat_features=categorical_cols)\n",
    "valid_data = cb.Pool(X_valid, label=y_valid, cat_features=categorical_cols)\n",
    "\n",
    "modelo = cb.CatBoostClassifier(verbose=0, loss_function='Logloss', eval_metric='AUC', random_seed=42,cat_features=categorical_cols)\n",
    "# Definir parámetros del modelo\n",
    "grid_params = {\n",
    "    'iterations': [100, 200, 300],  # Número de iteraciones\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Tasa de aprendizaje\n",
    "    'depth': [4, 6, 8],  # Profundidad del árbol\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],  # Regularización L2\n",
    "}\n",
    "\n",
    "best_params = modelo.grid_search(grid_params, X=X_train, y=y_train, cv=3, partition_random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m best_params\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_leaf_reg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m300\u001b[39m}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#solo_parametros = best_params[\"params\"]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m modelofinal \u001b[38;5;241m=\u001b[39m \u001b[43mcb\u001b[49m\u001b[38;5;241m.\u001b[39mCatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msolo_parametros, cat_features\u001b[38;5;241m=\u001b[39mcategorical_cols, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Entrenar con todo el dataset de entrenamiento\u001b[39;00m\n\u001b[1;32m     10\u001b[0m modelofinal\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m(X_valid, y_valid), early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cb' is not defined"
     ]
    }
   ],
   "source": [
    "#print(best_params)\n",
    "# best{'params': {'depth': 8, 'learning_rate': 0.1, 'l2_leaf_reg': 3, 'iterations': 300},\n",
    "best_params = {'params': {'depth': 8, 'learning_rate': 0.1, 'l2_leaf_reg': 3, 'iterations': 300}, 'cv_results': defaultdict(<class 'list'>, {'iterations': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], 'test-AUC-mean': [0.8853262094239304, 0.8890214633087871, 0.893074877154686, 0.8942308162366498, 0.8948925495319076, 0.8968689143830112, 0.8987713650717891, 0.8998583411874513, 0.900679856169647, 0.9012954374684198, 0.9018205262352352, 0.9022630887297686, 0.9027389782140315, 0.9031087687579668, 0.9035492938124549, 0.9039908496385486, 0.9045493493273401, 0.9049601886260122, 0.9053199690426362, 0.9056562579004472, 0.90597299620045, 0.9063187862693926, 0.906631319909439, 0.9068776163796933, 0.90715060608882, 0.9073794407507112, 0.9076155940490566, 0.907853584760329, 0.9080843279209674, 0.9082901183937179, 0.9084374375204449, 0.9085779227936867, 0.9087225332629517, 0.9088588105186842, 0.9089425990881473, 0.9090835478186414, 0.9091717076997209, 0.909242220589324, 0.9094070930014793, 0.9094838301260876, 0.9095761583029068, 0.9096381386608954, 0.9097234316433981, 0.9097958213450642, 0.9098691516403593, 0.9098986035553619, 0.909970579324496, 0.9100570648618179, 0.9101233384432815, 0.9101893526002153, 0.9102631962020066, 0.9103174045907781, 0.9103743523649496, 0.9104466368336236, 0.9104946038186955, 0.9105601031254094, 0.9106030535066005, 0.9107010467191436, 0.9107699093946184, 0.9107971691090778, 0.9108340296102907, 0.9108982320966943, 0.9109936927262501, 0.9110314479697922, 0.9110435862146448, 0.9110834842557809, 0.9111290266161767, 0.91115858185991, 0.9111761047419238, 0.9112011673377499, 0.9112505041213749, 0.9112593908708496, 0.9112771445492064, 0.9112864400834103, 0.9113267163089785, 0.91137271481958, 0.9113948621301292, 0.9114225867227993, 0.9114412426984506, 0.9114471765776392, 0.9114730207995329, 0.9115280538345992, 0.9115534820663916, 0.9115799267017254, 0.9116085748229943, 0.9116453389859588, 0.9116783443324245, 0.9116739777146563, 0.911665180592404, 0.9116943078654365, 0.9117089184434697, 0.9117133606377562, 0.9117309615279243, 0.9117425524603796, 0.9118033458072065, 0.9118282251429836, 0.911839184365399, 0.9118502915313339, 0.9118640301751949, 0.9118890206093591, 0.9119067759055568, 0.9119221920025103, 0.9119609793116723, 0.9119527402892819, 0.9120045218613031, 0.9120130558648127, 0.9120180861669187, 0.9120384631500299, 0.9120491060540696, 0.912080336085467, 0.9121104531743117, 0.9121018894459816, 0.9121459623677354, 0.91213921238969, 0.9121544604682451, 0.9121701852441452, 0.9121915895087943, 0.9122027364915075, 0.9122358114120046, 0.9122797317198694, 0.9123153003285122, 0.9123126357232452, 0.9123398498232808, 0.912364964404016, 0.91238852036926, 0.9124135205573287, 0.912423820672657, 0.9124370553798111, 0.9124575086076879, 0.9124462402521724, 0.9124820179425509, 0.9125336412633102, 0.9125570035772932, 0.912614414908203, 0.9126536277634933, 0.9126913243737763, 0.9126925005157958, 0.9127353284977877, 0.9127526523650521, 0.9127568006331689, 0.9127759762543644, 0.9127954010719354, 0.9128040143269754, 0.9128062977838244, 0.9127999760225091, 0.9128071539868747, 0.9128122215177307, 0.9128371620023694, 0.9128730517274973, 0.9128812673160634, 0.9129056338923184, 0.9129145714635122, 0.9129204540427525, 0.9129366406464238, 0.9129294203364456, 0.9129464595865433, 0.9129400922095109, 0.9129590398518065, 0.9129642844438393, 0.9129687989716992, 0.912987646323964, 0.9129924981911907, 0.913008989790141, 0.9130358219162239, 0.9130564686750832, 0.9131006165266963, 0.9131250496929004, 0.9131333901291153, 0.9131501008755948, 0.9131505997194153, 0.9131551782640196, 0.9131687353252648, 0.9131540668186315, 0.913156176799878, 0.9131784894095749, 0.9131783526653919, 0.9131913603700016, 0.9132103201989322, 0.9132155393214587, 0.9132163875785633, 0.9131965379145491, 0.9132206392973806, 0.9132361592555137, 0.9132416199369758, 0.9132472470256608, 0.9132557350853489, 0.9132624893548215, 0.9132493740718379, 0.9132601032350021, 0.9132724795252442, 0.9132914590396658, 0.91329369043331, 0.913281756317184, 0.9132830125483067, 0.913290129189909, 0.9132993116083491, 0.9132983198055031, 0.9133040466975335, 0.9133090501175314, 0.9133216307969548, 0.9133467725743728, 0.9133627420234185, 0.9133889800659566, 0.9133876035977635, 0.9134112274247856, 0.9134035507254371, 0.9133977739091508, 0.9134223091508608, 0.913434897478667, 0.9134403032229734, 0.913470684182874, 0.9134911084264045, 0.9134913409054536, 0.9134984500121863, 0.9135055734786267, 0.9135163477738071, 0.913529327131875, 0.9135263027939411, 0.9135477902670762, 0.9135577184667835, 0.9135663114784154, 0.9135869877420322, 0.9135713114673406, 0.913584181268258, 0.9136004179106939, 0.9135988597789035, 0.9135960363879357, 0.9135993449412122, 0.9136079644024866, 0.9136108046933377, 0.9136110568964781, 0.9136139401507499, 0.9136074463745848, 0.9135990662522165, 0.9135922527487882, 0.9136079200423822, 0.9136259702247896, 0.9136249440588875, 0.9136275227553202, 0.9136248627665013, 0.9136139967629514, 0.9136105607888206, 0.9136116479670898, 0.9136130188684451, 0.9135971813946222, 0.9136023488926771, 0.9135942960067741, 0.9135828813809939, 0.913594032798693, 0.9135815202565244, 0.9135881987550833, 0.9135874317905638, 0.9135987301932346, 0.9136175890423982, 0.9136075212164986, 0.9136016555351518, 0.9136062820069304, 0.9136131766429706, 0.9136214079297287, 0.9136447963530127, 0.9136515514969673, 0.9136639902326721, 0.9136618818230059, 0.9136625526424785, 0.9136520849406206, 0.9136546195048713, 0.9136560338401164, 0.9136529309247688, 0.9136487111984556, 0.9136591205627269, 0.9136563655325588, 0.9136564191883352, 0.9136676728955343, 0.9136686768997394, 0.9136673615939382, 0.9136658171281397, 0.9136800286450047, 0.9136856887642305, 0.9136762896433285, 0.9136852308672498, 0.9137067132900786, 0.9137239824070219, 0.9137166307397281, 0.9137297949737958, 0.913725166352722, 0.9137274952778496, 0.9137264178217489, 0.913720941386681, 0.9137095369486637, 0.9137048959793893, 0.913730560109166, 0.9137406399338714, 0.9137481797964154, 0.913731552799626, 0.9137302814254431, 0.9137303549288726, 0.9137420241120293, 0.9137394109092806, 0.9137443601368749, 0.9137321029059878], 'test-AUC-std': [0.002550368232615913, 0.0028144804517379733, 0.0023205535005025968, 0.0025588995583172604, 0.002759428658590408, 0.004158371332708203, 0.0033476506821079736, 0.002987492774222378, 0.0024741556223266755, 0.0022889604446249536, 0.0025761803778725292, 0.002695355271153409, 0.0025815229236703144, 0.0024390903523067624, 0.002332426548780466, 0.0026047356749592963, 0.0025793184736220593, 0.0026429753695298964, 0.002526931195375187, 0.0024180988859398554, 0.002418897854861972, 0.0024109624919168575, 0.0024650518481392687, 0.002433823646283693, 0.002465768367299788, 0.0025323460053245004, 0.002556890020618361, 0.0025866770275516275, 0.0025416197583277935, 0.0025664974943255064, 0.002563677170730499, 0.002503870816355784, 0.0024625538800626544, 0.0023763540290297515, 0.002318762617052779, 0.0023149034140200397, 0.0023339149011864795, 0.0023065903982350034, 0.0022927959922287456, 0.002281297742181716, 0.002265854994986699, 0.002241369108224221, 0.002209244114057247, 0.002188653764383161, 0.0021864156832790633, 0.0021613813890555785, 0.002150027012788009, 0.0021633990415900486, 0.002096738434119553, 0.0020874171632776165, 0.0021047769153177626, 0.0021206517573972303, 0.002142660564280479, 0.0021581321599488516, 0.0021889523687706035, 0.002183026136825601, 0.0021699637383725216, 0.0021948007934368447, 0.0021534274843453233, 0.002144344353980547, 0.0021654993240175155, 0.002204753296548782, 0.00222075939227672, 0.0021941602603726264, 0.002217714363850423, 0.002204659121123426, 0.002178470755240736, 0.002195292375350792, 0.0022114870231882016, 0.0022295816777101493, 0.0022379235730777794, 0.002251486736694554, 0.002240593763601258, 0.00225336935668543, 0.0022372984228935496, 0.002251803656930305, 0.0022751905927383895, 0.002247856134326762, 0.002239519258951532, 0.002227734931072113, 0.002224926708464146, 0.0022487454583073087, 0.002215947873487744, 0.002227487616917917, 0.002205600477020806, 0.0022055802520321334, 0.002164073621284584, 0.002170651462585786, 0.0021650825057358287, 0.0021613281905807115, 0.002148085761982641, 0.0021433302757538695, 0.0021251713593473903, 0.0021155063552557277, 0.0021194520353092178, 0.0020519060782183247, 0.002046423545187262, 0.0020349984811128797, 0.002028249927855566, 0.0020141331719386794, 0.001997876656957827, 0.0019850833257765263, 0.0019839149327891543, 0.0019913315768146133, 0.0019236859459103282, 0.0019053090268401256, 0.0019060818040965435, 0.0019081660826794668, 0.0018815492341088685, 0.0018854141313824646, 0.0019323103549663727, 0.0019410019020616068, 0.0019997532752703546, 0.0019832442835177534, 0.001960841851536776, 0.0019388331248794486, 0.0019490438668255158, 0.0019583409541196885, 0.0019709337275571736, 0.002011910413917279, 0.001985137359537764, 0.001984678046398035, 0.001982283141139296, 0.0019733500218723165, 0.0019428989980793785, 0.0019417901907224028, 0.0019371003189075737, 0.001953204818658036, 0.0019648618436390636, 0.0019666105400632347, 0.001978824296741143, 0.001984777205532362, 0.0019835057023571395, 0.002001246285471001, 0.002055585235206253, 0.002007142999345093, 0.0019936238012319744, 0.001994368284289652, 0.0019715620739596904, 0.001922697588868924, 0.0019180402508653077, 0.0019334468572326283, 0.0019513011051025043, 0.0019249797083621268, 0.0019242627969938823, 0.0019206940672769112, 0.0019262197641304294, 0.0019180769746332787, 0.0019256789828100821, 0.0019392154267140803, 0.001941088525576544, 0.0019299431181953686, 0.0019283183241124832, 0.0019349354684753525, 0.0019176342271985927, 0.0018935835817963736, 0.0019013509093415285, 0.0018748242930679891, 0.001893724925196645, 0.0019345267836448834, 0.001933964284334904, 0.0019389517606236662, 0.0019363026663396247, 0.0019166055181729313, 0.0019454419852722375, 0.0019049802593304106, 0.0018782255775445613, 0.0018355577545262284, 0.0018824814757631805, 0.0018905105455463513, 0.0019045235769158601, 0.0018749697072272006, 0.001877289455928516, 0.0018950738072988328, 0.0019011588900354646, 0.0018705065097351142, 0.0018681424957086169, 0.0018525173806709272, 0.0018517660219289852, 0.0018440267107210757, 0.0018352804453534717, 0.0018599965817138588, 0.001841162712611127, 0.0018332006219335863, 0.0018372064629106483, 0.001842677433177094, 0.0018584112105266427, 0.0018657548857186454, 0.0018662088029188216, 0.0018673220967236179, 0.0018474401302823068, 0.0018558834187671562, 0.0018637746124896756, 0.0018475155207433567, 0.0018394539915301388, 0.0018358070270358003, 0.0018250526722466843, 0.0018268474439546267, 0.001822378810502871, 0.0018155224881176157, 0.0018171480110348284, 0.001811131135255376, 0.0018100016143458304, 0.0017924176046993363, 0.0017583840102922991, 0.0017654194889972243, 0.0017832395255428647, 0.0017651203376867998, 0.0017631315866915363, 0.0017531466733967187, 0.0017800362448407577, 0.0017496781006845433, 0.0017557727433588782, 0.0017435059117977844, 0.001759097253508614, 0.0017624867105700897, 0.0017527283614411915, 0.0017541210694751745, 0.0017532624995758275, 0.0017482524990776113, 0.001730891617232848, 0.0016873914545154226, 0.001693193512583736, 0.0016957324466037942, 0.0017204539418664362, 0.0017175060316969011, 0.0017207956910023218, 0.0017267796283341036, 0.001736555154829577, 0.001722783593682454, 0.0017138764636578246, 0.0017321217003701704, 0.0017415846024645592, 0.0017331029274328647, 0.0017237927094521399, 0.0017027327146038637, 0.0016833922695570999, 0.0016849999950769802, 0.0016843225512475765, 0.0016816063126358551, 0.0016944060016995087, 0.0016975933607003711, 0.0016985222169338498, 0.0016845308869657511, 0.0016900018484529219, 0.0016900798622113925, 0.0016894993985978113, 0.0016860555138751058, 0.0016742984828214344, 0.00165761003194598, 0.001654436740555659, 0.001639938239721129, 0.0016385739682747613, 0.001634936622640766, 0.0016461446648844077, 0.0016436040144963754, 0.0016125730398147172, 0.0016109919198910173, 0.0016074079385470156, 0.0016105024929108573, 0.0016066964526425015, 0.00162162269781427, 0.00162960651606345, 0.0016176909209389234, 0.0016076786642089064, 0.0016023120080248545, 0.0015973372502574173, 0.0015973912269642652, 0.0015958401575578687, 0.0015994867960175644, 0.0015993933594131346, 0.0015957380039425134, 0.0016047943521284467, 0.0015964210831061114, 0.0015961279927904648, 0.0016134099059716327, 0.001606739004444733, 0.0016185431987038074, 0.0016482671977796422, 0.0016500440789693849, 0.0016289209968663556, 0.0016116611783620425, 0.0016131691653768771, 0.0016453225978423414, 0.0016369412564654865, 0.0016408482987061645, 0.0016531962469200424, 0.0016444209293405934, 0.0016870881757199616, 0.001683381815738588, 0.0016741694542819513, 0.0016579390477450095, 0.0016775386674059958, 0.001655808292676169, 0.0016496073882864464, 0.0016170642381292976, 0.0016015562079521035, 0.0016105359270728427, 0.0016170763191474444, 0.0016136721070761845], 'test-Logloss-mean': [0.6225457717916605, 0.5600248299713994, 0.5218938504110054, 0.49177496852410657, 0.47041174642629824, 0.45231075391701664, 0.43836142194949, 0.4278002109545523, 0.42053532311602737, 0.4144420910799301, 0.40854546162376265, 0.4044290716296322, 0.40086078615001663, 0.3981070216559044, 0.39517734336902316, 0.3927359581916749, 0.39044391135171036, 0.38849063746854257, 0.3868955892010453, 0.385584726638638, 0.3843617452269783, 0.3832083485898905, 0.3820836490696169, 0.3811976235398828, 0.38031045532610913, 0.379590396795976, 0.37890684154024384, 0.3782275994968442, 0.3775788802025377, 0.37699253138592853, 0.3765790742275172, 0.3761674587328778, 0.37581911093613557, 0.3753984772410833, 0.37509596287032837, 0.37474614721099436, 0.374506611656464, 0.3742927607806565, 0.37392116954128, 0.3737275428153423, 0.3734616560838129, 0.37328396845479817, 0.3730681334301566, 0.3728528872035972, 0.3726828556935778, 0.3726084965773162, 0.37242955633566144, 0.3722130001498703, 0.37202498881576007, 0.3718961871747326, 0.37170515558207523, 0.3715522879497619, 0.3714384271189772, 0.3712859894078924, 0.37119018576973967, 0.3710428130082715, 0.37092552001581874, 0.37072165881916685, 0.3705532758973454, 0.3704728991603414, 0.37038587489703545, 0.3702538733750564, 0.3700565745488438, 0.36998512670780936, 0.36992917849511775, 0.36981774508967497, 0.3697230691610052, 0.3696569164650272, 0.3696099294023001, 0.3695623388442499, 0.3694566333852751, 0.3694167279893091, 0.3693768329845872, 0.3693517115094724, 0.36927635671958114, 0.36916947804376044, 0.3691243419589303, 0.3690515445248292, 0.3690119356701272, 0.36898373909049703, 0.36893617363092535, 0.3688312474084947, 0.3687735988455663, 0.36870113669905463, 0.36862784643842555, 0.36855792219522715, 0.3684921050125128, 0.36849045658323054, 0.3684977431572362, 0.3684306451994485, 0.3683901868389117, 0.3683693078581669, 0.36832432209354754, 0.36830456497648295, 0.3681965065915962, 0.36814164233950325, 0.36811485617780937, 0.3680879720958794, 0.36806744156671106, 0.3680144496905469, 0.367976955546007, 0.367943889370465, 0.3678757609727634, 0.36787415522654826, 0.3677764078520663, 0.3677563597318449, 0.3677435452629136, 0.36771022161614825, 0.36769054373827686, 0.3676349393992422, 0.367581618429232, 0.3675817123002217, 0.367502120715876, 0.3675029075811557, 0.36746767630567173, 0.3674346870500609, 0.3673964334061928, 0.3673646888518845, 0.36731311801636907, 0.36721199474559346, 0.3671363830788845, 0.3671264618206238, 0.3670618375557794, 0.36701813718824344, 0.36697160306964854, 0.3669274854624332, 0.3669077093738518, 0.3668855203876848, 0.36684370978998965, 0.36685046484318273, 0.36677476678054766, 0.3666731516511433, 0.3666098063008187, 0.36651296599662225, 0.36644746763833197, 0.36636530246348054, 0.36636210456547164, 0.3662752570238646, 0.36624744165777673, 0.36624307651503085, 0.3662032513450298, 0.36616914209792584, 0.3661495888755309, 0.3661380712441844, 0.3661375817705332, 0.3661184266093965, 0.3661022349160962, 0.36604946805989647, 0.36598249952615564, 0.36596547531238444, 0.3659029259092768, 0.36587981893247185, 0.365869569081801, 0.36584686938696676, 0.36585481912984075, 0.36582014707833066, 0.36582751320696394, 0.3657804216952493, 0.36576077722162664, 0.36574951508050163, 0.3657147257179912, 0.3657076584908781, 0.36567435415839133, 0.3656270225962015, 0.3655835766573298, 0.36548699114102234, 0.36544573477489145, 0.3654261232062459, 0.36538840791562593, 0.365393116775439, 0.36537785209944323, 0.36534362085125266, 0.3653670016201029, 0.36535408471858727, 0.36531032491897025, 0.3653047410097619, 0.3652732341195201, 0.36523680114297513, 0.3652311585378314, 0.36523262069407575, 0.36527137558805317, 0.365221014694856, 0.365192147898972, 0.36517507040387026, 0.36516928536999366, 0.3651477737063497, 0.36514261123392927, 0.365165260198374, 0.36514638842479913, 0.3651296671352832, 0.36509364710731534, 0.3650838406800399, 0.3651097797102949, 0.36511594602273983, 0.36509216782419013, 0.36507402737037803, 0.3650774720373207, 0.3650558245088315, 0.36504500875462526, 0.36501460874835406, 0.3649704700740298, 0.3649440162586446, 0.3648845983857993, 0.36488256392442836, 0.36484363947831416, 0.3648495915560868, 0.3648636654271146, 0.36482061041784847, 0.364792629633472, 0.364777908847484, 0.36472216177659894, 0.36468591420859425, 0.3647061821494823, 0.36469677006331463, 0.3646800438393269, 0.36465213809109476, 0.36461887400085996, 0.36462206071866393, 0.36458064312903055, 0.36456751893894385, 0.3645566834134457, 0.3645091081027531, 0.3645373588987009, 0.3645235536348606, 0.3644900363870698, 0.36448906157259015, 0.3644925703127775, 0.36447164097324114, 0.36445485642568354, 0.36444125522699977, 0.36443340346061626, 0.36442936109334173, 0.364435599537587, 0.36444562736167035, 0.36446673069284685, 0.36444964092951854, 0.36441345781000267, 0.3644023068167999, 0.3644043360141182, 0.3644070796944612, 0.3644274832048677, 0.36442393955334546, 0.3644211776136244, 0.36441733754588973, 0.36444767921329974, 0.3644304941942866, 0.36444235508930706, 0.36446351630258517, 0.3644359136574236, 0.3644505863445131, 0.3644356754795925, 0.36443328288432464, 0.3644148559232177, 0.364381550259258, 0.3643806524190816, 0.36438770782504765, 0.3643703813523092, 0.36435155241597944, 0.364326388741365, 0.3642803217771566, 0.3642666411500728, 0.36424477632119956, 0.36424872214829, 0.3642497751495622, 0.36426845223964266, 0.36426498364948895, 0.3642635080631577, 0.36426609560087825, 0.36426538456814894, 0.3642480437632556, 0.3642546697981656, 0.364252122656322, 0.3642324779497754, 0.3642357111533203, 0.36424802206335966, 0.36424517465262146, 0.3642188663256803, 0.3642052453661235, 0.36421147051409103, 0.3641911659099935, 0.3641528807293548, 0.364113384050009, 0.3641286325821958, 0.3641043365628877, 0.36411195770360627, 0.3641148388088377, 0.36412118689097533, 0.3641376719757485, 0.3641615146708277, 0.3641680362668221, 0.36411388043333526, 0.364100309285011, 0.36408065964628955, 0.3641137631199664, 0.364103499590948, 0.36410563947203695, 0.36408446379238085, 0.3640945013004105, 0.3640868654920211, 0.3641087711044168], 'test-Logloss-std': [0.0001048165616652943, 0.0004066378299074493, 0.0017668421508299104, 0.001701611484528254, 0.0008300248003539489, 0.002133963671423614, 0.0016578856963647838, 0.002221573883720116, 0.001886482742541705, 0.0022455590120926683, 0.002571578135146151, 0.002955184745221037, 0.002796723624505641, 0.002736735756303453, 0.0028132134442610977, 0.003580613167763091, 0.0035589228500716194, 0.0037644642041173054, 0.0036273139356902823, 0.003471964715711691, 0.003539080959951772, 0.0037137356487360277, 0.0038761865075326043, 0.003941263928701338, 0.004074663313201633, 0.0041945033966933176, 0.004320155700891787, 0.004364026525919896, 0.004264083608106578, 0.004327430873026048, 0.004388169920034081, 0.0042683143548376875, 0.004213545772217757, 0.004056570849991165, 0.0038978580554664528, 0.0038706258690305263, 0.0039174757261693305, 0.0038690580361967425, 0.003871544285629604, 0.0038406601009353548, 0.0038111278926820495, 0.00379132728431114, 0.0037621536682465786, 0.0037404743250590415, 0.003758533423511777, 0.0037242960100760825, 0.003714785846619813, 0.0037454821718070318, 0.0036290589749055206, 0.0036287256866259967, 0.003669060235823372, 0.003707886067737825, 0.0037451672937504413, 0.0037839030951023513, 0.0038505900921345436, 0.0038250870156094066, 0.0037861593748195837, 0.0038426654749516603, 0.003750419996218532, 0.0037413384359337625, 0.003783740508332743, 0.0038433529351077026, 0.0038618227729574476, 0.0038208579321176513, 0.003850424308894088, 0.0038431293543731778, 0.0037914769938385677, 0.0038507497818204123, 0.0038676008126295636, 0.003907998160535663, 0.003926713915893618, 0.003960099562354343, 0.003944567094997121, 0.0039373015010536775, 0.003910431439608702, 0.00394797136583652, 0.00398570080338003, 0.003940296612635301, 0.0038992766035371184, 0.0038852512515604905, 0.0038790569063781727, 0.00392975657797784, 0.003868275969566436, 0.003897519053389175, 0.003854082706621003, 0.0038519730821762236, 0.0037886752304872933, 0.003783985848466022, 0.0037728634289976287, 0.0037703198610080065, 0.0037409558934144603, 0.0037305636661408453, 0.0036950322608948673, 0.003682790558014143, 0.0036852547579858913, 0.0035503025819280634, 0.003549667719864937, 0.00352940184484864, 0.0035132454836284232, 0.0034875038028635823, 0.003457426398931134, 0.0034378979284442886, 0.0034198894854024, 0.0034485696084698187, 0.0033365365315654676, 0.003311947262543663, 0.0033193235051159203, 0.0033240632195323423, 0.003286521961612174, 0.0033025754916354362, 0.003376469890516503, 0.0033876755516026444, 0.0034969901257721836, 0.00345064982377786, 0.0034122299803207795, 0.0033800045815425587, 0.00339974498311133, 0.0034095988175814377, 0.0034331093901419523, 0.003503134063108299, 0.0034599830505079093, 0.0034858273175169888, 0.003471468051357355, 0.003452959143704341, 0.0033908406311097977, 0.0033945745905778254, 0.003382084690406918, 0.0034179767619722436, 0.003436420978616292, 0.0034258239551699854, 0.0034418060510024774, 0.003452500949723613, 0.0034338756076984136, 0.0034631401370884556, 0.003547678112133048, 0.003462346625167137, 0.0034399205878897204, 0.0034376580405722826, 0.0034095507322916448, 0.003322551554313838, 0.0033158385091492174, 0.0033454416799835727, 0.003370653026170067, 0.003327964530735729, 0.003323698878838952, 0.0033118434258539546, 0.003329123703225648, 0.00331128895025981, 0.0033288252228590477, 0.003356753441545523, 0.0033403945110111392, 0.003320320675123483, 0.0033129917368212042, 0.0033195691332801058, 0.003287202765402887, 0.003234859764568603, 0.0032496535901214985, 0.0032150008514870414, 0.0032528464001556944, 0.0033125793613539584, 0.003305250474640953, 0.003317309781123544, 0.003305572416615113, 0.003269653962280953, 0.00332831267596329, 0.003253489314164466, 0.0032075693880635253, 0.003126643148045612, 0.003205049948449777, 0.0032215454662127864, 0.0032440312995679317, 0.0031971020345131465, 0.0032021734407738918, 0.003235684254953726, 0.003240307643979208, 0.0031842310061642384, 0.003199233362617124, 0.00317636515920975, 0.0031798591252639634, 0.0031741093750253697, 0.0031522392561051642, 0.0031980116643440613, 0.0031564312878475448, 0.003136870838384374, 0.0031447195445437706, 0.0031460760066381733, 0.0031758426979352325, 0.0031958287988440416, 0.0031959028457446046, 0.003203672337764003, 0.0031684249106125836, 0.0031809809644374023, 0.003192431781546389, 0.0031611697397304333, 0.003155800976284767, 0.003147624735227803, 0.0031353456663634517, 0.003148683612773749, 0.0031388720120119357, 0.0031225736193738664, 0.0031311781773182405, 0.003120735106294778, 0.003123333414531328, 0.003087925888435457, 0.00303094721028541, 0.00303200704724287, 0.003061460351445359, 0.0030288749038855567, 0.0030291683696267242, 0.003007215560289884, 0.0030544645112859817, 0.002999842795650667, 0.0030270122324753857, 0.0030229301007073765, 0.003047461428419122, 0.00305383925989715, 0.0030349039466124224, 0.0030498197228652304, 0.0030725971868729583, 0.003053098815706394, 0.0030162661916495314, 0.00292560066013976, 0.002937071410354237, 0.0029483619535667833, 0.002996990297955373, 0.002984384689359099, 0.0030019110856225498, 0.0030140051278617027, 0.003034769583038084, 0.0030116659521987436, 0.0029967681895107655, 0.003025620683566873, 0.0030534403450005573, 0.0030382352066568104, 0.0030174347896409735, 0.0029861245887682956, 0.0029661862254379626, 0.002962609335448476, 0.002948023041180306, 0.0029360802497034005, 0.0029622456930223614, 0.002961960999002463, 0.002963492841382827, 0.0029334738014473692, 0.002942574280295562, 0.002939224855131285, 0.0029375793476179163, 0.002936893573394279, 0.0029081489450023664, 0.0028772008277041334, 0.0028790521519182987, 0.0028522783278922092, 0.002842742599806575, 0.002841742436304276, 0.002831784271257563, 0.0028250815502172465, 0.0027674720893148914, 0.0027593807578576894, 0.0027562576356611407, 0.0027577456432991087, 0.002751435826609922, 0.002767425660412554, 0.0027737062042029656, 0.0027555117898007075, 0.002730119043618222, 0.0027192302098345417, 0.0027126851907352313, 0.00271009433734332, 0.0027053604428324907, 0.002711517903769939, 0.002711436123338314, 0.0026987848615652745, 0.002714652697827546, 0.002696005460539654, 0.0026978285232960877, 0.002726674502852741, 0.002714411964734578, 0.002737499906181025, 0.0027982668757274994, 0.0028022444224216686, 0.002756879809550545, 0.002717613594625372, 0.0027284614561679872, 0.0027872909146579522, 0.002775805883912605, 0.002790761996686611, 0.0028159705344082595, 0.0028111561814217756, 0.002886569631594723, 0.0028829849148253083, 0.0028603588985114665, 0.0028325600919610954, 0.0028667917182518595, 0.0028188726875544813, 0.0028089912413976938, 0.002756818982157848, 0.0027300895311284145, 0.0027463122607950413, 0.002760786172509297, 0.002756736368389142], 'train-Logloss-mean': [0.6228145751982911, 0.5604358548858228, 0.52224907567521, 0.4925763849154324, 0.47133132875580713, 0.4533502749088716, 0.43946584405403777, 0.4290006835154932, 0.42153768519546997, 0.41536328471711687, 0.40935720483197074, 0.4052107952743487, 0.401664303629901, 0.3987605434626566, 0.39577594622363704, 0.393230125755938, 0.391002000419484, 0.38902415347149794, 0.38738322363196437, 0.38609383807333447, 0.38484322812281047, 0.38367092247253937, 0.3825662024463634, 0.3816314680752422, 0.38074284013921544, 0.3798676011561028, 0.3790325491924205, 0.37830431926837177, 0.3776334962978683, 0.37689367363212267, 0.3762600122749444, 0.37565698569037614, 0.37526335449251946, 0.3746134323594778, 0.37411125059982653, 0.3734643406513358, 0.3730227219799154, 0.3726128743098798, 0.3721581537005068, 0.3717396061593836, 0.3711757206899446, 0.37086715434335976, 0.3704572471840761, 0.370085062173608, 0.36969565395809806, 0.36946257978020314, 0.36914065569803123, 0.3686769856935089, 0.3682025512257236, 0.3677883170395881, 0.36745206990463225, 0.366957324065671, 0.3666642366983683, 0.36633834168770907, 0.3660924147974753, 0.3657026036931675, 0.36540475983742177, 0.36502987394793934, 0.3645549070455454, 0.36427386288146657, 0.36397494757391086, 0.3636534335834248, 0.3633548174361443, 0.3630756984235766, 0.36282288200772417, 0.3624399668291265, 0.3622120209035626, 0.36193189134482234, 0.36163329147604234, 0.3614302655247091, 0.36115408004929533, 0.36083959556401934, 0.36057842146243685, 0.3603044661904851, 0.36004865600966185, 0.35971092120925235, 0.3595616402263233, 0.35912654441697295, 0.3589083600667831, 0.35867630021133695, 0.35849724606274896, 0.35818258396566466, 0.3579170086181733, 0.3577369888197752, 0.357459578110421, 0.35719536150634595, 0.3569053815840681, 0.35667995040033434, 0.3564327856493968, 0.3562261810861281, 0.3560915922494876, 0.35594064726494756, 0.3557362767977463, 0.3555228463316955, 0.35524770380947474, 0.35502703056574153, 0.3548545298905212, 0.3546153463617305, 0.3544680320959035, 0.3541950457345651, 0.3539220781964358, 0.3537582686479377, 0.35349004798323963, 0.3532343108866603, 0.35292914923797564, 0.3526725738270758, 0.35238471966098633, 0.3521829269668035, 0.3519049648813665, 0.35158648653658414, 0.3513763319756757, 0.3512049109470199, 0.35093283379374046, 0.35073736495072866, 0.35044007380989256, 0.35012851915837356, 0.3498971563485796, 0.3497237452552222, 0.349517416860893, 0.3492492557123384, 0.34897143438229333, 0.3487629454554603, 0.3485449487696619, 0.34834981611490906, 0.3480665370272571, 0.34780894516986754, 0.3475926073036892, 0.3473240281650511, 0.3470830152435191, 0.3468232906361746, 0.34659843272437446, 0.34636855994074917, 0.3461525043916142, 0.34593482579057017, 0.34569108380470775, 0.3454153723963879, 0.3450871114430285, 0.34479857212457166, 0.3445812884099584, 0.3442815770309052, 0.3441363636558597, 0.3439389688706192, 0.34370588599570473, 0.3434992045902865, 0.34324189855702764, 0.3429819010262549, 0.34275100411804793, 0.3425468553524014, 0.34237063809395024, 0.34220105163854636, 0.3419896164089739, 0.3417943383372459, 0.3416556405461993, 0.3414444933311272, 0.3412005296547595, 0.34099703074080095, 0.3408874817560654, 0.3406516004849444, 0.3404346352485659, 0.34022813410808844, 0.3400512521209347, 0.33981544987120244, 0.33970787991374735, 0.339519312911831, 0.33931446918109437, 0.3390978708168959, 0.33890997921489446, 0.33872559164332894, 0.3384177519794708, 0.33821537741369334, 0.33801163378659904, 0.33776933228039385, 0.3375685181526129, 0.33734990335632925, 0.33721099025451906, 0.3369386110788959, 0.3367058225156432, 0.3364236292275648, 0.33627102729312836, 0.3360914592987638, 0.3358732448810981, 0.33570360151088363, 0.3355214779776891, 0.33534792910651584, 0.33520033089220463, 0.33504540184131776, 0.33488264137016727, 0.3347165083889025, 0.3345660731495344, 0.3344244308099724, 0.3342825379407944, 0.334176250957486, 0.3339550052059989, 0.33370787108349065, 0.3335160731438119, 0.33332286353852186, 0.3331934394460827, 0.33305164284739036, 0.33295835321430534, 0.3327293241582658, 0.3325663881684651, 0.3323955495720536, 0.33213002814565734, 0.33194581941520035, 0.3318188165856588, 0.3316465366948403, 0.3314995031889843, 0.33130993525447844, 0.33114391564858503, 0.3309859470816298, 0.3307802337088202, 0.3306332403310433, 0.33039251984807444, 0.33025574439655325, 0.3301237581927379, 0.3299423640345034, 0.3297089538392927, 0.3294867559757965, 0.329257082441073, 0.3291130150123867, 0.3289709590208972, 0.32877708350518464, 0.3286271885752547, 0.3284506938536085, 0.32824348023926625, 0.3280634448192556, 0.3279090713429475, 0.3276640240116348, 0.3275558855491249, 0.32737706031224856, 0.32712672526288666, 0.32696036403674783, 0.32680145680008543, 0.32658271135241446, 0.32646359344034376, 0.32621430098246434, 0.326044576792746, 0.32588544765601263, 0.3257477997430939, 0.3256245713516593, 0.3255003078530784, 0.32534831083459537, 0.32522065218550283, 0.32507741540483664, 0.32491560670286807, 0.32471256515520397, 0.32448860730797896, 0.3244124192615852, 0.3242898274198934, 0.3241148825433261, 0.32393435273847976, 0.32374089107577725, 0.3235817892892567, 0.3234214323488869, 0.3231822025164531, 0.3230166016006144, 0.32279782237940385, 0.3226450306249618, 0.32249262648387683, 0.3223433041773361, 0.3222107014339022, 0.32200877117664123, 0.32182893049434713, 0.3215979174010439, 0.32144834223295254, 0.3212410442299831, 0.3211347017333397, 0.32104349758299, 0.32095623921652244, 0.3208386442890588, 0.3207165411546666, 0.32063666066473034, 0.3204949544815312, 0.32030635555412257, 0.3201500229089758, 0.32003641874380073, 0.3199111397724326, 0.3197983431567788, 0.31961341477849076, 0.319441590097941, 0.3192448026383938, 0.3190929592445248, 0.3189047720501023, 0.3187379975945398, 0.3185095874539275, 0.3183398640545054, 0.31816828420120497, 0.3179722442430694, 0.317827348321927, 0.31779683297648204, 0.31764026556433655, 0.3174639788770895, 0.31734540378102594, 0.3172146260456823, 0.3170544283834195, 0.3168254542771369, 0.3166811651706496, 0.31648979771404123, 0.3162944905805231, 0.3161701162432398], 'train-Logloss-std': [0.00035805420929711253, 0.0008907942812930165, 0.0032647550590567415, 0.0032764419923364675, 0.0030796857099437444, 0.0034243886543054536, 0.002817904769180078, 0.0012546953075572842, 0.0021844376452667055, 0.0024292870124049135, 0.0017799722680100938, 0.001211258044594798, 0.0013310361806758426, 0.001513664702276687, 0.001534438311939207, 0.0010388597927642913, 0.0010990644924173554, 0.0010713116564108702, 0.001180771287119091, 0.0012605447235188576, 0.0013701681274241775, 0.0012402097642289505, 0.0009918253361811398, 0.0009711237213503656, 0.0008728305580492208, 0.000903944856141013, 0.000968936645446642, 0.0009359156860992092, 0.0009872877629129105, 0.0009055638387999371, 0.0008341649075672487, 0.0009664026066660501, 0.0010316434907148594, 0.0011422720813356918, 0.0013502662258264761, 0.0013010821383200647, 0.0013285480340852493, 0.0013796976072848945, 0.0014054580371791363, 0.001553519575863349, 0.0015437912266408154, 0.001569066026146812, 0.0014765377168430873, 0.0015545392520094074, 0.0016602815441565846, 0.001669482285210625, 0.0017180009367414864, 0.0016028249330558811, 0.0017447334347139795, 0.001817918861785414, 0.001800520677595819, 0.0017639122804364935, 0.0017025873715532565, 0.0016199978046244174, 0.0016181648074285303, 0.0016539462837274616, 0.00164155179309281, 0.0017043146202881823, 0.0016586087891971451, 0.0017237770822439156, 0.0017215201413350737, 0.001731135934957553, 0.0017085191317902624, 0.001759973019144869, 0.0016719078093665684, 0.0016982421181826626, 0.001688681862319749, 0.0016169684771273347, 0.0017058462105996796, 0.0016126180174208223, 0.0015902390917828968, 0.0014542002391159032, 0.0013650263430970515, 0.0015598187823353963, 0.001471962107392741, 0.0013721481861800784, 0.001382442661319621, 0.001250248568878666, 0.0011556632359611013, 0.0011862365293663264, 0.001186706833239148, 0.0011140397249475953, 0.001244395553695201, 0.0012472181910194806, 0.0012337092416498268, 0.001238422985122553, 0.001111123794685314, 0.0011426628263781864, 0.0011705964326659145, 0.0012616719415928764, 0.0012808770774868188, 0.0012543846282198005, 0.0012484762827945353, 0.0012415717049223205, 0.0011870559416728742, 0.0012434583455246152, 0.001291893488151831, 0.0012637335637713297, 0.0012816796671655678, 0.0014743859317588289, 0.0015579502294385036, 0.0016338237635012641, 0.0015358333259232288, 0.0015315573584788902, 0.001570155841965806, 0.001588573935938078, 0.0015249689343824937, 0.0016069294340541821, 0.0015880856182798658, 0.0015436800319973151, 0.0015203240714293704, 0.0015501141272227996, 0.0014723705211927352, 0.0015512764739709463, 0.0015956956581121234, 0.0017660544252770716, 0.0016556782000360496, 0.0016172145343991942, 0.0015608475264512318, 0.0014862905559088623, 0.0015082043333211048, 0.0014306533969533682, 0.0013752076680900208, 0.0014121764039214833, 0.0015625029784847755, 0.0015225439355489655, 0.0015978540810088506, 0.001517045301696346, 0.0015594888860280406, 0.0016182719273690614, 0.0016085098867417145, 0.0015460835206776474, 0.001497141887830341, 0.0015325465773005982, 0.0014814220563456127, 0.0014967584229228297, 0.0015640268014889536, 0.0015549552112160308, 0.0016022339719772636, 0.0016802436484389795, 0.001622191666666553, 0.001601321082018925, 0.0015101136353318483, 0.0015529259400837332, 0.0015442724927335627, 0.0015507773923568227, 0.0015848237024981675, 0.001562293026352224, 0.0015557929120202177, 0.0014904186392328104, 0.001426286144618065, 0.0014421558341632084, 0.0014423878216895177, 0.0014313762647977664, 0.0014033418206332324, 0.0013831436445802647, 0.0013851189090300022, 0.0013966260655048316, 0.0014230386849966915, 0.0014000166483858012, 0.001389536114426291, 0.0013590033401314435, 0.0013219468830785272, 0.0014774666929494658, 0.0014461370903062717, 0.0015211413344493035, 0.0016157676074482251, 0.00158770756855241, 0.001585118989285098, 0.0016171837544412323, 0.0016110265342358782, 0.0016694659998115113, 0.0016277800735297544, 0.0016590895155768942, 0.001602304907562724, 0.0015475175852206811, 0.0014635070499492498, 0.0014652143760507275, 0.001456838674925953, 0.0014538908316698788, 0.0014783335090542853, 0.0015118115147542519, 0.0015248346320652746, 0.0014784144152875478, 0.001441011851709618, 0.0014882168653885185, 0.001543886820744295, 0.0014955442725280274, 0.001565574637785868, 0.0016289820788149803, 0.0016743280984760276, 0.0016688598456368775, 0.0016683319144816734, 0.0016094630632579204, 0.0016022683785700484, 0.0015793507191632329, 0.001564125958742008, 0.0016245800791936807, 0.0016068881204895206, 0.0014919469640072606, 0.0015204802902670676, 0.001572938138228281, 0.0014709041412430347, 0.0014812188539004678, 0.0015370746905683313, 0.0015518053006646861, 0.0016263299909066158, 0.001609735045795691, 0.0015673439093613455, 0.001601015517387423, 0.001629941785078205, 0.0016861946497850925, 0.0017699239883050676, 0.0017954505731940015, 0.0017563495196090862, 0.001649527154918007, 0.0016770211395847905, 0.0016446233701120634, 0.0016346371479540867, 0.0016938518241810213, 0.0017365901905072652, 0.0017979540901493548, 0.0018522210828186636, 0.0018741325373157459, 0.001877711456841174, 0.0018922783381685512, 0.00197962234037135, 0.0021428952690088084, 0.0021689032118339894, 0.002190526571877327, 0.002108136595702181, 0.0021116640531036344, 0.002042192569304144, 0.001977021311583422, 0.0020082041849077263, 0.002022927648285419, 0.0019916587113339686, 0.0019301043682866529, 0.001954398583915827, 0.002022799616543759, 0.0020487336813300814, 0.0021029514629563187, 0.0020611708704135387, 0.0020658464172545973, 0.0020770475128433067, 0.0020276471838243087, 0.0019499778442345803, 0.0019061519435760906, 0.0019185565459461835, 0.0018791626505269016, 0.0018616272613251157, 0.0017765311514045077, 0.0017146308244277548, 0.0017474090442384624, 0.0016782736652937308, 0.0016438021835277258, 0.0015861304600468475, 0.0015056636107972552, 0.001414819208774846, 0.0014432165289424322, 0.0014528571001263199, 0.0014076209498871276, 0.0013443617876439757, 0.0014071643947648083, 0.0013293498926965134, 0.001263733018238634, 0.0013496232298156928, 0.0013280486531417237, 0.0013377542097803645, 0.0012893459073397998, 0.0012841484275326726, 0.0012691723915641798, 0.0012651582280517925, 0.001277767091135224, 0.0013113420281627972, 0.0013498784690358302, 0.001328499327920065, 0.0013081493859405523, 0.0013377138202577796, 0.0013331719648423865, 0.0013400742401061309, 0.0013772544488174104, 0.0012860186099454623, 0.0012897814704447602, 0.0013173356847520385, 0.0013023808855495513, 0.0012758489727799573, 0.0012507817010279578, 0.001306504049146406, 0.0012826311783809645, 0.0013239268108748003, 0.0013236271075434834, 0.001366644156798173, 0.0013049661918301553, 0.0012844456599632188, 0.0013361587626109462, 0.0013430085998855559, 0.0013200096128497666, 0.0013428814867024283, 0.0013003209910121493]})}\n",
    "\n",
    "solo_parametros = best_params[\"params\"]\n",
    "# Entrenar el modelo\n",
    "\n",
    "modelofinal = cb.CatBoostClassifier(**solo_parametros, cat_features=categorical_cols, verbose=100)\n",
    "\n",
    "# Entrenar con todo el dataset de entrenamiento\n",
    "modelofinal.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50)\n",
    "\n",
    "# Probar distintos hiperparametros\n",
    "\n",
    "\n",
    "# Predecir probabilidades en el conjunto de prueba\n",
    "y_pred_proba = modelofinal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calcular la Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar la Curva ROC\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC - CatBoost\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_predicciones \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m: canciones\u001b[38;5;241m.\u001b[39miloc[X_test\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTARGET\u001b[39m\u001b[38;5;124m'\u001b[39m: modelofinal\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m })\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Guardar en un archivo CSV\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df_predicciones\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicciones_catboost.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_predicciones = pd.DataFrame({\n",
    "    'ID': canciones.iloc[X_test.index, 0],\n",
    "    'TARGET': modelofinal.predict_proba(X_test)[:, 1]\n",
    "})\n",
    "\n",
    "# Guardar en un archivo CSV\n",
    "df_predicciones.to_csv(\"predicciones_catboost.csv\", index=False)\n",
    "\n",
    "print(\"Predicciones guardadas en 'predicciones_catboost.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_predicciones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_predicciones\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_predicciones' is not defined"
     ]
    }
   ],
   "source": [
    "print(df_predicciones.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
